{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relatório do TP1 de RI\n",
    "\n",
    "#### Alunos:    \n",
    " - Alexandre Luis Ribeiro Martins\n",
    " - Diego Santos Gonçalves\n",
    " - Gabriel Arrighi Silva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principais desafios, decisões e arquitetura utilizada\n",
    "\n",
    "\n",
    "O coletor foi desenvolvido com base no repositório público fornecido pelo professor Daniel Hasan Dalip. Os principais desafios encontrados foram os seguintes:\n",
    "\n",
    "\n",
    " - Detalhes em alguns testes: Para algumas etapas em específico, o teste automatizado possuía alguns problemas que impossibilitavam garantir o bom funcionamento do coletor. Para essas etapas, foi necessário avaliar e corrigir os testes.\n",
    " - Coordenação entre threads: Durante a etapa final, foi necessário debugar e corrigir detalhes no código que só ficava evidente ao testar por meio da execução de diversas threads, em alguns momentos o código ficava em loop e, em outros, coletava menos do que deveria. A solução foi encontrada após depurar o código e identificar detalhes nas etapas anteriores que não tinham sido implementados como deveriam.\n",
    " - Criação da página de documentação: Houve um certo desafio na decisão de qual plataforma utilizar para a página. Por fim, a solução mais simples e barata foi utilizar o github.io.\n",
    "\n",
    "\n",
    "A maior parte das decisões tomadas seguem o modelo padrão do projeto esperado. Alguns detalhes foram:\n",
    " - Diminuímos o tempo de espera ao ficar sem novos domínios para coletar para 1 segundo, visto que o valor utilizado (30s) era desnecessário.\n",
    " - Decidimos utilizar None na resposta de exceção do `request_url`\n",
    "A arquitetura utilizada segue a arquitetura padrão do projeto, com a diferença que, no método run do `page_fetcher`, utilizamos um loop dentro de um bloco de exceção, para evitar que uma falha crítica em uma thread interrompa o trabalho das demais. Além disso, foi utilizada a estrutura padrão fornecida, com as classes `Scheduler` e `PageFetcher` com suas funções atribuídas e a chamada para execução dentro do notebook fornecido. Maiores detalhes da implementação se encontram no item 3 deste relatório. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs sementes utilizadas\n",
    "\n",
    "Para a coleta, foram selecionadas as seguintes URLs sementes:\n",
    "\n",
    " - [https://www.terra.com.br ](https://www.terra.com.br )\n",
    " - [https://www.uol.com.br](https://www.uol.com.br) \n",
    " - [https://www.canaltech.com.br](https://www.canaltech.com.br) \n",
    " - [https://www.casasbahia.com.br](https://www.casasbahia.com.br) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como foi feito, faça referências à classes e métodos do código fonte:\n",
    "\n",
    "Inicialmente, temos a classe `Domain` que representa os domínios do sistema. Cada servidor será representado por um domínio e as funções da classe irão controlar o tempo de requisição de acesso. A classe `Scheduler`, o escalonador, percorrerá a lista de servidores e obterá as URLs dos servidores acessíveis, controlando a fila. Por fim, foi implementada a classe `PageFetcher`, que controlam as threads responsáveis por fazer as requisições das URLs vindas do `Scheduler`.  \n",
    "\n",
    " - Os critérios de exclusão de robôs e quantidade de tempo entre requisições à um mesmo servidor:\n",
    "\n",
    "O critério de exclusão de robôs foi implementado na Classe `Scheduler`, no método `can_fetch_page`. Nesse método, foi utilizado o módulo de `robotparser` da `urllib`, que estabelece as regras de exclusão de robôs de acordo com o arquivo `robots.txt`. Há um dicionário de configuração do `robots.txt` em que a chave é o cada domínio novo descoberto. Então, para validar a URL, verifica-se no item do dicionário cuja chave é o domínio da URL e chama o método `can_fetch`.\n",
    "\n",
    "A quantidade de tempo entre as requisições a um mesmo servidor foi implementada na Classe `Scheduler`, e o tempo do intervalo é armazenado no atributo `TIME_LIMIT_BETWEEN_REQUESTS` em milisegundos. No método `get_next_url`, esse limite de tempo também é utilizado caso o dicionário de URL descobertas for vazia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O impacto na velocidade de coleta, a quantidade de páginas por segundo, aumenta o número de threads de 1 a 20 de 5 em 5 passos e, logo após, de 30 a 100 com 20 passos . Ao fazer o estudo, colete menos páginas (~100 páginas visitadas pode ser o suficiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawler.scheduler import Scheduler\n",
    "from crawler.page_fetcher import PageFetcher\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "arr_seeds = [\"https://www.terra.com.br\",\n",
    "            \"https://www.uol.com.br\",\n",
    "            \"https://www.canaltech.com.br\",\n",
    "            \"https://www.casasbahia.com.br\"]\n",
    "\n",
    "arr_seeds = [urlparse(str_url) for str_url in arr_seeds]\n",
    "\n",
    "arr_results = []\n",
    "\n",
    "def test_crawler(num_threads):\n",
    "\n",
    "    scheduler = Scheduler(usr_agent=\"OtakuBot (https://allrightishere.github.io/OtakuBot/)\", page_limit=100, depth_limit=10, arr_urls_seeds=arr_seeds)\n",
    "\n",
    "    arr_fetcher = []\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(num_threads):\n",
    "        arr_fetcher.append(PageFetcher(scheduler))\n",
    "        arr_fetcher[i].start()\n",
    "\n",
    "\n",
    "    for fetcher in arr_fetcher:\n",
    "        fetcher.join()\n",
    "    end = time.time()\n",
    "\n",
    "    interval = end - start\n",
    "\n",
    "    arr_results.append((num_threads, interval))\n",
    "\n",
    "arr_threads = [ x*5 for x in range(4)]\n",
    "arr_threads[0] = 1\n",
    "\n",
    "arr_threads += [ x*20+30 for x in range(4)]\n",
    "arr_threads.append(100)\n",
    "\n",
    "for t in arr_threads:\n",
    "    test_crawler(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(arr_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link da página\n",
    "\n",
    "O coletor ciado possui uma página documentativa que é utilizada no User-Agent durante as coletas, a página pode ser vista através do link: \n",
    "[https://allrightishere.github.io/OtakuBot/](https://allrightishere.github.io/OtakuBot/) .\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
